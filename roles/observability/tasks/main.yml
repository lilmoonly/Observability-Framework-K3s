---
# tasks file for observability role to deploy the observability stack

- name: Ensure Helm is installed on master
  become: true
  delegate_to: k8s-ctrl
  ansible.builtin.shell: |
    curl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash
  args:
    creates: /usr/local/bin/helm

- name: Add Helm repos for OpenSearch, Prometheus, CloudNativePG, and Fluent-bit
  become: true
  delegate_to: k8s-ctrl
  ansible.builtin.shell: |
    export KUBECONFIG=/etc/rancher/k3s/k3s.yaml
    /usr/local/bin/helm repo add opensearch https://opensearch-project.github.io/helm-charts/
    /usr/local/bin/helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
    /usr/local/bin/helm repo add cnpg https://cloudnative-pg.github.io/charts
    /usr/local/bin/helm repo add fluent https://fluent.github.io/helm-charts
    /usr/local/bin/helm repo update

- name: Taint worker nodes for observability workloads
  become: true
  delegate_to: k8s-ctrl
  ansible.builtin.shell: |
    export KUBECONFIG=/etc/rancher/k3s/k3s.yaml
    kubectl taint nodes app-worker workload=app:NoSchedule --overwrite
    kubectl taint nodes db-worker workload=db:NoSchedule --overwrite
    kubectl taint nodes logging-node workload=logging:NoSchedule --overwrite
    kubectl taint nodes monitor-node workload=monitoring:NoSchedule --overwrite
    kubectl taint nodes ai-node workload=ai:NoSchedule --overwrite
  register: taint_result
  failed_when: "taint_result.rc != 0 and 'already has' not in taint_result.stderr"

- name: Deploy OpenSearch via Helm
  become: true
  delegate_to: k8s-ctrl
  community.kubernetes.helm:
    kubeconfig: /etc/rancher/k3s/k3s.yaml
    name: opensearch
    chart_ref: opensearch/opensearch
    release_namespace: opensearch
    create_namespace: true
    wait: yes
    timeout: 1200s
    values:
      persistence:
        enabled: true
        storageClass: "{{ storage.storage_class }}"
      singleNode: true
      securityConfig:
        enabled: true
      extraEnvs:
        - name: OPENSEARCH_INITIAL_ADMIN_PASSWORD
          value: "{{ opensearch.admin_password }}"
        - name: node.store.allow_mmap
          value: "false"
        - name: OPENSEARCH_JAVA_OPTS
          value: "-Xmx{{ opensearch.heap_size }} -Xms{{ opensearch.heap_size }}"
      nodeSelector:
        workload: "{{ opensearch.node_selector_label }}"
      tolerations:
        - key: "workload"
          operator: "Equal"
          value: "{{ opensearch.node_selector_label }}"
          effect: "NoSchedule"
      resources:
        requests:
          memory: 512Mi
          cpu: 200m
        limits:
          memory: 2Gi
          cpu: 1000m
      startupProbe:
        tcpSocket:
          port: 9200
        initialDelaySeconds: 120
        periodSeconds: 10
        failureThreshold: 60
      readinessProbe:
        tcpSocket:
          port: 9200
        initialDelaySeconds: 60
        periodSeconds: 10
        failureThreshold: 10

- name: Deploy OpenSearch Dashboards via Helm
  become: true
  delegate_to: k8s-ctrl
  community.kubernetes.helm:
    kubeconfig: /etc/rancher/k3s/k3s.yaml
    name: opensearch-dashboards
    chart_ref: opensearch/opensearch-dashboards
    release_namespace: opensearch
    create_namespace: true
    wait: yes
    timeout: 1200s
    values:
      opensearchHosts: "https://opensearch-cluster-master:9200"
      config:
        opensearch_dashboards.yml:
          server.host: "0.0.0.0"
          opensearch.ssl.verificationMode: none
          opensearch.username: "admin"
          opensearch.password: "{{ opensearch.admin_password }}"
          opensearch.requestHeadersWhitelist: [ "authorization", "securitytenant" ]
      service:
        type: LoadBalancer
        port: "{{ opensearch_dashboards_port }}"
      nodeSelector:
        workload: "logging"
      tolerations:
        - key: "workload"
          operator: "Equal"
          value: "logging"
          effect: "NoSchedule"
      resources:
        limits:
          memory: 512Mi
          cpu: 500m
      startupProbe:
        tcpSocket:
          port: 5601
        initialDelaySeconds: 60
        periodSeconds: 10
        failureThreshold: 20

- name: Deploy Fluent-bit via Helm
  become: true
  delegate_to: k8s-ctrl
  community.kubernetes.helm:
    kubeconfig: /etc/rancher/k3s/k3s.yaml
    name: fluent-bit
    chart_ref: fluent/fluent-bit
    release_namespace: opensearch
    wait: yes
    values:
      config:
        outputs: |
          [OUTPUT]
              Name  es
              Match *
              Host  opensearch-cluster-master.opensearch.svc.cluster.local
              Port  9200
              HTTP_User admin
              HTTP_Passwd {{ opensearch.admin_password }}
              tls On
              tls.verify Off
              Index fluent-bit
              Suppress_Type_Name On
      tolerations:
        - operator: "Exists"
          effect: "NoSchedule"

- name: Download Grafana dashboards
  delegate_to: localhost
  run_once: true
  vars:
    dashboards_dir: "{{ role_path }}/files/dashboards"
  loop: "{{ grafana_dashboards }}"
  loop_control:
    loop_var: dashboard_id
  get_url:
    url: "https://grafana.com/api/dashboards/{{ dashboard_id }}/revisions/1/download"
    dest: "{{ dashboards_dir }}/dashboard_{{ dashboard_id }}.json"
    mode: '0644'

- name: Create ConfigMaps for Grafana dashboards
  become: true
  vars:
    dashboards_dir: "{{ role_path }}/files/dashboards"
  loop: "{{ grafana_dashboards }}"
  loop_control:
    loop_var: dashboard_id
  kubernetes.core.k8s:
    kubeconfig: /etc/rancher/k3s/k3s.yaml
    state: present
    template: grafana-dashboard-cm.j2

- name: Deploy kube-prometheus-stack via Helm
  become: true
  delegate_to: k8s-ctrl
  community.kubernetes.helm:
    kubeconfig: /etc/rancher/k3s/k3s.yaml
    name: prometheus
    chart_ref: prometheus-community/kube-prometheus-stack
    release_namespace: monitoring
    create_namespace: true
    wait: yes
    timeout: 1200s
    values:
      prometheus:
        prometheusSpec:
          retention: "{{ monitoring.prometheus_retention }}"
          storageSpec:
            volumeClaimTemplate:
              spec:
                storageClassName: "{{ storage.storage_class }}"
                accessModes: [ "ReadWriteOnce" ]
                resources:
                  requests:
                    storage: "{{ storage.prometheus_size }}"
          resources:
            requests:
              memory: 512Mi
              cpu: 200m
            limits:
              memory: 1Gi
              cpu: 1000m
      grafana:
        adminPassword: "{{ monitoring.grafana_admin_password }}"
        service:
          type: LoadBalancer
          port: "{{ grafana_port }}"
        defaultDashboardsEnabled: false
        sidecar:
          dashboards:
            enabled: true
            label: grafana_dashboard
            labelValue: "1"
            searchNamespace: ALL
      nodeSelector:
        workload: "{{ monitoring.node_selector_label }}"
      tolerations:
        - key: "workload"
          operator: "Equal"
          value: "{{ monitoring.node_selector_label }}"
          effect: "NoSchedule"

- name: Install CloudNativePG operator via Helm
  become: true
  delegate_to: k8s-ctrl
  community.kubernetes.helm:
    kubeconfig: /etc/rancher/k3s/k3s.yaml
    name: cnpg-operator
    chart_ref: cnpg/cloudnative-pg
    release_namespace: cnpg-system
    create_namespace: true
    wait: yes

- name: Create db namespace
  become: true
  delegate_to: k8s-ctrl
  kubernetes.core.k8s:
    kubeconfig: /etc/rancher/k3s/k3s.yaml
    state: present
    definition:
      apiVersion: v1
      kind: Namespace
      metadata:
        name: db

- name: Create app-db-secret in db namespace
  become: true
  delegate_to: k8s-ctrl
  kubernetes.core.k8s:
    kubeconfig: /etc/rancher/k3s/k3s.yaml
    state: present
    definition:
      apiVersion: v1
      kind: Secret
      metadata:
        name: app-db-secret
        namespace: db
      type: Opaque
      data:
        username: "{{ 'appuser' | b64encode }}"
        password: "{{ 'appuserpassword' | b64encode }}"

- name: Deploy PostgreSQL Cluster resource (HA)
  become: true
  delegate_to: k8s-ctrl
  kubernetes.core.k8s:
    kubeconfig: /etc/rancher/k3s/k3s.yaml
    state: present
    template: "postgres-cluster.yaml.j2"
    wait: yes